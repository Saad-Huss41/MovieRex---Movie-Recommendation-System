{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+yEetDNe//3O7OTbU61PC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saad-Huss41/MovieRex---Movie-Recommendation-System/blob/main/Logic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df1=pd.read_csv('/content/tmdb_5000_credits.csv')\n",
        "df2=pd.read_csv('/content/tmdb_5000_movies.csv')\n",
        "# Just reading the files and importing some libraries we'll need."
      ],
      "metadata": {
        "id": "oxDKVNSYB6AW",
        "outputId": "2af258db-2369-4012-d00e-e65b21df7cb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/tmdb_5000_credits.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5b728822e60>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/tmdb_5000_credits.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/tmdb_5000_movies.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Just reading the files and importing some libraries we'll need.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/tmdb_5000_credits.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns = ['id','title','cast','crew']\n",
        "df2= df2.merge(df1,on='id')\n",
        "df2.head(5)\n",
        "# Concatenate the datasets."
      ],
      "metadata": {
        "id": "mKhDOlkpwr77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C= df2['vote_average'].mean()\n",
        "C"
      ],
      "metadata": {
        "id": "_dBcFe-LOzlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m= df2['vote_count'].quantile(0.9)\n",
        "m"
      ],
      "metadata": {
        "id": "u4NAIWpVxN-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_movies = df2.copy().loc[df2['vote_count'] >= m]\n",
        "q_movies.shape\n",
        "#q_movies are \"qualified movies\", movies with a certain number (m) of votes."
      ],
      "metadata": {
        "id": "6Psa9AAOO2jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_rating(x, m=m, C=C):\n",
        "    v = x['vote_count']\n",
        "    R = x['vote_average']\n",
        "    return (v/(v+m) * R) + (m/(m+v) * C)\n",
        "# Calculation based on the IMDB formula\n"
      ],
      "metadata": {
        "id": "NCJLOq0ExTw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_movies['score'] = q_movies.apply(weighted_rating, axis=1)"
      ],
      "metadata": {
        "id": "V2io0YPWxWog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_movies = q_movies.sort_values('score', ascending=False)\n",
        "q_movies[['title', 'vote_count', 'vote_average', 'score']].head(10)\n",
        "# Here is our first very basic recommender system."
      ],
      "metadata": {
        "id": "8TEzaYW1QNlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Tfidf = Term Frequency-Inverse Document Frequency. TfidfVectorizer turns docs into matrices with numerical values showing how important a word is to the doc."
      ],
      "metadata": {
        "id": "PGqTQxM-xjPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(stop_words='english')"
      ],
      "metadata": {
        "id": "BbPd8w7ex0ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['overview'] = df2['overview'].fillna('')\n",
        "# We want to fill any empty descriptions with an empty string."
      ],
      "metadata": {
        "id": "VdLmvFJXx2Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix = tfidf.fit_transform(df2['overview'])\n",
        "# Here we just make the matrix"
      ],
      "metadata": {
        "id": "8g_o9WCrx7b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix.shape"
      ],
      "metadata": {
        "id": "4KIrJZDAx-lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import linear_kernel"
      ],
      "metadata": {
        "id": "18zSzY6TzCND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "# The great thing about using linear_kernel here is that it'll perform the same thing as Cosine Similarity except it wont take as long.\n",
        "# As for what it does, it calculates the dot product of the two vectors.\n",
        "# Basically it multiplies the two vectors by each other which will tell us how similar each element is to every other element in the Matrix"
      ],
      "metadata": {
        "id": "6E1MmLcrzHdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim.shape"
      ],
      "metadata": {
        "id": "mTziUyp4zV3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Construct a reverse map of indices and movie titles\n",
        "indices = pd.Series(df2.index, index=df2['title']).drop_duplicates()\n"
      ],
      "metadata": {
        "id": "27PptjaU0Vlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices[:10]"
      ],
      "metadata": {
        "id": "-gqad74o0Y2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We define a function to return movies with similar scores\n",
        "def get_recommendations(title, cosine_sim=cosine_sim):\n",
        "    idx = indices[title]\n",
        "\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    # Sort from most similar to least\n",
        "\n",
        "    sim_scores = sim_scores[1:11]\n",
        "    # 10 closest scores\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    return df2['title'].iloc[movie_indices]\n"
      ],
      "metadata": {
        "id": "AnC_-dKm0am5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_recommendations('The Great Gatsby'))"
      ],
      "metadata": {
        "id": "6-WTdhXZ0kp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Congratulations!! You have a working recommender system based with *Content Based Filtering* and specifically centered around the plot overview."
      ],
      "metadata": {
        "id": "OrPY1uCbyH0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "import ast"
      ],
      "metadata": {
        "id": "TCult87-0lsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Important note for using other datasets: cast_y and crew_x should be changed to cast or crew if thats how the columns are named in the dataset.\n",
        "# In my case, I named them this way to make this code block run since I had both cast_y, cast_x and crew_y,crew_x in my df2.\n",
        "features = ['cast_y', 'crew_x', 'keywords', 'genres']\n",
        "for feature in features:\n",
        "  df2[feature] = df2[feature].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') and x.endswith(']') else x)\n",
        "  # We want to validate the list is present before we add it to avoid any errors."
      ],
      "metadata": {
        "id": "wLxCIzGuZodJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_director(x):\n",
        "    for i in x:\n",
        "        if i['job'] == 'Director':\n",
        "            return i['name']\n",
        "    return np.nan\n",
        "    # We are getting the director of the movie."
      ],
      "metadata": {
        "id": "iUtwXGNjZstt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_list(x):\n",
        "    if isinstance(x, list):\n",
        "        names = [i['name'] for i in x]\n",
        "        #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.\n",
        "        if len(names) > 3:\n",
        "            names = names[:3]\n",
        "        return names\n",
        "\n",
        "    #Return empty list in case of missing/malformed data\n",
        "    return []"
      ],
      "metadata": {
        "id": "khUT0CUGb4Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['director'] = df2['crew_x'].apply(get_director)\n",
        "\n",
        "features = ['cast_y', 'keywords', 'genres']\n",
        "for feature in features:\n",
        "    df2[feature] = df2[feature].apply(get_list)"
      ],
      "metadata": {
        "id": "f-1Cb2srb-Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2[['title', 'cast_y', 'director', 'keywords', 'genres']].head(3)"
      ],
      "metadata": {
        "id": "4K-UKlpscqYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(x):\n",
        "    if isinstance(x, list):\n",
        "        return [str.lower(i.replace(\" \", \"\")) for i in x]\n",
        "    else:\n",
        "        #Check if director exists. If not, return empty string\n",
        "        if isinstance(x, str):\n",
        "            return str.lower(x.replace(\" \", \"\"))\n",
        "        else:\n",
        "            return ''"
      ],
      "metadata": {
        "id": "F6L6w_MRcsck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['cast_y', 'keywords', 'director', 'genres']\n",
        "\n",
        "for feature in features:\n",
        "    df2[feature] = df2[feature].apply(clean_data)\n",
        "    # Clean the dataset into lower case and get rid of spaces."
      ],
      "metadata": {
        "id": "GNg_g6nLfAaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_soup(x):\n",
        "    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast_y']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])\n",
        "df2['soup'] = df2.apply(create_soup, axis=1)\n",
        "# We are making metadata \"soup\" which is a string with all the data we want to feed our vectorizer."
      ],
      "metadata": {
        "id": "0ilVQwDZfHDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count = CountVectorizer(stop_words='english')\n",
        "count_matrix = count.fit_transform(df2['soup'])\n",
        "# Count vectorizer returns how many times each word appears in a text in the form of a matrix."
      ],
      "metadata": {
        "id": "7eMaM7ztfK9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cosine_sim2 = cosine_similarity(count_matrix, count_matrix)\n",
        "# We are now performing Cosine Similarity which checks how similar two vectors are by measuring the Cosine angle between them.\n",
        "# Typically used in text document similarity analysis."
      ],
      "metadata": {
        "id": "RgpkGNxEfYuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2.reset_index()\n",
        "indices = pd.Series(df2.index, index=df2['title'])"
      ],
      "metadata": {
        "id": "Cy51hF0sfayy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_recommendations('The Dark Knight Rises', cosine_sim2)"
      ],
      "metadata": {
        "id": "qXGvUAMyfcw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_recommendations('The Godfather', cosine_sim2)\n",
        "# Congratulations again! You have another recommender system based on Keywords, Genres, and Credits."
      ],
      "metadata": {
        "id": "mczHk9Nrfgpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install surprise"
      ],
      "metadata": {
        "id": "2x6SSsUZfkMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import Reader, Dataset, SVD\n",
        "from surprise.model_selection import cross_validate\n",
        "reader = Reader()\n",
        "ratings = pd.read_csv('/content/ratings_small.csv')\n",
        "ratings.head()\n",
        "# Reader() reads ratings"
      ],
      "metadata": {
        "id": "CYRw-eU8lOon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = Reader(rating_scale=(1, 5))"
      ],
      "metadata": {
        "id": "xteUaYGfny4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n"
      ],
      "metadata": {
        "id": "zX_MFAscl8um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svd = SVD()\n",
        "cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
        "# A bit to unpack here: SVD() is Single Value Decompisition. We use this to decrease the dimensionality of the matrix by taking the latent factors.\n",
        "# RMSE is the Root Mean Square Error, a performance indicator for regression models. The lower, the better\n",
        "# It works by measuring the average difference between values predicted by a model and the actual values. (measures accuracy.)\n",
        "# MAE = Mean Absolute Error. It calculates the average magnitude of the absolute errors between the predicted and actual values.\n",
        "# cv is Cross Validaton Iterator. It determines how the data will be split. For further information refer to sklearn's documentation: KFold — scikit-learn 1.6.dev0 documentation\n",
        "# verbose=True just tells it to print the accuracy measure for each split."
      ],
      "metadata": {
        "id": "N8zgk3q5nmWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iedoCJxAm6L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = data.build_full_trainset()\n",
        "svd.fit(trainset)\n",
        "# We build our training set from which we can query predictions.\n",
        "# .fit() actually did the training"
      ],
      "metadata": {
        "id": "iuRW0NN7n8er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings[ratings['userId'] == 1]"
      ],
      "metadata": {
        "id": "E76cJN-QoReG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svd.predict(1, 302, 3)"
      ],
      "metadata": {
        "id": "uKdVXGDNqlb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For one final time, congratulations!!! I hope this helped you out and was not too difficult of a read. I tried to make this tutorial easy for beginners to understand, but I think any confusion can be solved with reading the documentation.\n",
        "# Thank you for reading!"
      ],
      "metadata": {
        "id": "JpJJ9U_fqnl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "conn = sqlite3.connect('movierex.db')"
      ],
      "metadata": {
        "id": "4TPnXiSzF9Lp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS users (\n",
        "    user_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    name TEXT NOT NULL,\n",
        "    email TEXT NOT NULL UNIQUE,\n",
        "    preferences TEXT,\n",
        "    top5_movies TEXT\n",
        ")\n",
        "''')\n",
        "\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "_KVj_icjF92f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_user(name, email, preferences, top5_movies):\n",
        "    try:\n",
        "        cursor.execute('''\n",
        "        INSERT INTO users (name, email, preferences, top5_movies)\n",
        "        VALUES (?, ?, ?, ?)\n",
        "        ''', (name, email, preferences, top5_movies))\n",
        "        conn.commit()\n",
        "        print(f\"User {name} added successfully.\")\n",
        "    except sqlite3.IntegrityError as e:\n",
        "        print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "y7uAizTjGJNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_recommendations(user_id):\n",
        "    cursor.execute('SELECT top5_movies FROM users WHERE user_id = ?', (user_id,))\n",
        "    result = cursor.fetchone()\n",
        "\n",
        "    if not result:\n",
        "        print(\"User not found.\")\n",
        "        return []\n",
        "\n",
        "    top5_movies = result[0].split(', ')\n",
        "\n",
        "    recommendations = set()\n",
        "    for movie in top5_movies:\n",
        "        recommendations.update(get_movie_recommendations(movie, 5))\n",
        "\n",
        "    recommendations.difference_update(top5_movies)\n",
        "\n",
        "    return list(recommendations)[:5]"
      ],
      "metadata": {
        "id": "On0pDdWkGLtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    name = input(\"Enter your name: \")\n",
        "    email = input(\"Enter your email: \")\n",
        "    preferences = input(\"Enter your movie preferences (comma-separated genres): \")\n",
        "\n",
        "    print(\"Enter your top 5 favorite movies (one per line):\")\n",
        "    top5_movies = [input(f\"Movie {i+1}: \") for i in range(5)]\n",
        "    top5_movies_str = \", \".join(top5_movies)\n",
        "\n",
        "    add_user(name, email, preferences, top5_movies_str)\n",
        "\n",
        "    user_id = cursor.lastrowid  # Get the last inserted user ID\n",
        "    recs = generate_recommendations(user_id)\n",
        "\n",
        "    print(f\"\\nTop 5 movie recommendations for you: {recs}\")\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "JwKllNwJGPXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o5ll2J41GSCn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}